import os
import shutil
import numpy as np
import tensorflow as tf
import time

# Import modules from lib directory
from lib.STpconvUnet_chl import STpconvUnet
from lib.DataGenerator_chl_nc import DataGenerator
import kerastuner as kt

print("Using TensorFlow version", tf.__version__)
import matplotlib.pyplot as plt

from tensorflow.python.framework.ops import disable_eager_execution

gpus = tf.config.list_physical_devices('GPU')
print(f"Tensorflow version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

if gpus:
    try:
        # # Currently, memory growth needs to be the same across GPUs
        # for gpu in gpus:
        #     tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)


print("Starting the training")
print("...")

# Hyperparameters and other options
BATCH_SIZE = 2
N_EPOCHS = 250 #300
OUT_PATH = os.getcwd()
DATA_PATH_TRAINING = "../CHL_DATA/InputData_RIGA/TrainData_NC_RIGA"
DATA_PATH_VALIDATION = "../CHL_DATA/InputData_RIGA/ValData_NC_RIGA"

model_name = "model_203_RIGA_dataperc30-100"

N_CONV_LAYERS = 4
N_CONV_PER_BLOCK = 1
KERNEL_SIZES = [(3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)]
DILATION_RATE = 1
N_FILTERS    = [16, 32, 64, 128] #, 128
STRIDES = [(2, 2, 2),(2, 2, 1),(2, 2, 1), (2, 2, 2)] #, (2, 2, 2)


trn_generator = DataGenerator(DATA_PATH_TRAINING, batch_size = BATCH_SIZE)
val_generator = DataGenerator(DATA_PATH_VALIDATION, batch_size = BATCH_SIZE)

class MyHyperModel(kt.HyperModel):
    def build(self, hp):
        model = STpconvUnet(n_conv_layers = N_CONV_LAYERS, nx=160, ny=160, nt=20, 
                    kernel_sizes = KERNEL_SIZES, 
                    n_filters = N_FILTERS, 
                    learning_rate=hp.Float("learning_rate", min_value=1e-5, 
                                        max_value=1e-2, sampling="log"), 
                    n_conv_per_block=N_CONV_PER_BLOCK, 
                    dilation_rate=DILATION_RATE, 
                    strides = STRIDES,
                    act=hp.Choice("act", ["LeakyReLU", "tanh"]),
                    l1=hp.Float("l1", min_value=1e-5, max_value=1e-2, sampling="log"),
                    l2=hp.Float("l2", min_value=1e-5, max_value=1e-2, sampling="log"))
        return model.model

    def fit(self, hp, model, *args, **kwargs):
        return model.fit(
            *args,
            **kwargs,
        )
        
tuner = kt.BayesianOptimization(
    MyHyperModel(),
    objective="val_loss",
    max_trials=50,
    overwrite=True,
    directory="tuning_models_pCONV3D",
    project_name="tune_hypermodel_25_epochs",
)
        
    
cp_callback = [tf.keras.callbacks.ModelCheckpoint(filepath="./tuning_models_pCONV3D/tune_hypermodel_25_epochs",
                                                 save_best_only=True),
               tf.keras.callbacks.EarlyStopping('val_loss', patience=10)]


print("The search begins...")
tuner.search(trn_generator,
             validation_data=val_generator,
             epochs=25,
             callbacks=cp_callback)

best_model = tuner.get_best_models(1)[0]

best_hyperparameters = tuner.get_best_hyperparameters(1)[0]

print(best_hyperparameters)









model = STpconvUnet(n_conv_layers = N_CONV_LAYERS, nx=160, ny=160, nt=20, 
                    kernel_sizes = KERNEL_SIZES, n_filters = N_FILTERS, 
                    learning_rate=0.001, n_conv_per_block=N_CONV_PER_BLOCK, 
                    dilation_rate=DILATION_RATE, strides = STRIDES) #, loss="loglike"
model.summary()

# save model at the end of each epoch
checkpoint_dir = os.path.join(OUT_PATH, model_name) 
# Model name convention:

# first letter: 0 - data as-is, 1 - normalized (min-max), 2 - log-transformed, 3 - boxcox transformed
# second letter: 0 - batch size 2, 1 - batch size 4, 2 - batch size 8, 3 - batch size 10
# third lettter: order of strides 0 - 221,221,222,222 ; 1 - 222,222,221,221 ; 2 - 221,222,222,221 , 3 - 222,221,221,222

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="loss", patience=60)

# Reduce learning rate when a metric has stopped improving.

# Models often benefit from reducing the learning rate by a factor of 2-10 
# once learning stagnates. This callback monitors a quantity and if no 
# improvement is seen for a 'patience' number of epochs, the learning rate is reduced.

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor="loss", patience=30)

if os.path.exists(checkpoint_dir):
    shutil.rmtree(checkpoint_dir)
os.mkdir(checkpoint_dir)
checkpoint_filepath = os.path.join(checkpoint_dir, "epoch_{epoch:02d}.h5")
model_checkpoint_callback = [tf.keras.callbacks.ModelCheckpoint(
    filepath = checkpoint_filepath,
    save_weights_only=True,
    save_best_only=True),
    early_stopping,
    reduce_lr]

# save hyperparameters in a JSON file
model.save(os.path.join(OUT_PATH,f"{model_name}_architecture"), save_weights = False)

# time the execution of the model training
start_time = time.time()
# train model
history = model.model.fit(x=trn_generator, validation_data = val_generator, 
                        epochs = N_EPOCHS, callbacks=model_checkpoint_callback)
print("Training took", time.time() - start_time, "seconds")

loss = history.history['loss']
print(loss)
val_loss = history.history['val_loss']
print(val_loss)



#losses = [0.021444928184969396, 0.021829163769214618, 0.015117645059235213, 0.011016613630600637, 0.01127133757818672, 0.010349192606177317, 0.008785572652591438, 0.009438856127217593, 0.008965359347668, 0.00868611183108353, 0.008855007336156942, 0.00891577153716509, 0.008403892193000945, 0.007786270991967219, 0.007527615558412835, 0.007517810795234698, 0.007603797557616107, 0.008185576720634566, 0.008037153835175559, 0.007869593696895896, 0.007936046871913188, 0.007943642550721629, 0.00757788933058292, 0.00788929900608775, 0.007580976118333638, 0.008191737777939656, 0.007966175652676967, 0.008025781328153864, 0.00846280523280545, 0.009189790035967102, 0.008137481160058708, 0.007502571885508099, 0.007358306506719645, 0.007488784738021289, 0.007361174224614643, 0.007177630079506964, 0.00728643021302116, 0.007380403799754454, 0.007904022020605824, 0.007345909583552673, 0.00816561150216929, 0.007853089955283284, 0.00794754221058655, 0.007352833126433103, 0.007625990470576032, 0.007740247253578429, 0.007577101681551828, 0.007672590283086387, 0.00739338712641833, 0.00771620927276317]
#val_losses = [0.010789214487886056, 0.014338724222034216, 0.009014991950243712, 0.006934920558705926, 0.010725156287662685, 0.0071222806291189045, 0.006143609172431752, 0.006484717025887221, 0.007713395892642439, 0.00792038645595312, 0.006634794018464163, 0.005976196736446582, 0.005506358155980706, 0.006304502789862454, 0.00568677675910294, 0.005752452957676723, 0.0055610362411243845, 0.006762572133447975, 0.0071278362767770885, 0.006810366851277649, 0.006219798047095537, 0.005790111480746418, 0.006871153018437326, 0.005382029816973955, 0.005496995369321666, 0.006301868171431124, 0.005739780177827924, 0.006549380673095584, 0.0062521673971787095, 0.006200107024051249, 0.005374977923929691, 0.00541409042198211, 0.006083521875552833, 0.005260852808714844, 0.005455459852237254, 0.0052995393751189114, 0.005825907795224339, 0.006072636775206775, 0.0048968186369165775, 0.005579903171746991, 0.006799104996025563, 0.0061572642007377, 0.005616471989196725, 0.006120622542221099, 0.006378684367518872, 0.005994013976305723, 0.00574103023391217, 0.005313764838501811, 0.005614107777364552, 0.005370854030479677]
#loss = [0.0220371031645322, 0.02021764243509984, 0.018627824024694813, 0.017841922397520846, 0.018460884979903336, 0.01754837954365762, 0.016809867493376682, 0.01667364455458549, 0.01665896422899805, 0.016324842325979617]
#val_loss = [0.02006171699613333, 0.011622374829312321, 0.013391062431037427, 0.013923266367055476, 0.012772558780852705, 0.013398197176866234, 0.013309618597850204, 0.011463922448456287, 0.010867246799170971, 0.01221849232679233]
fig, ax = plt.subplots(2,1, sharex=True, sharey=True)
ax[0].plot(loss)
ax[1].plot(val_loss)
ax[0].set_title("Training Loss")
ax[1].set_title("Validation Loss")
plt.xlabel("number of epochs")
fig.text(-0.001, 0.5, "loss (MAE for artifically gapped pixels)", va='center', rotation='vertical')
plt.suptitle("Loss development")
plt.show()

# save the figure
fig.savefig(f"./loss_model_pics/{model_name}.png", dpi=300)

#%%
import matplotlib.pyplot as plt
loss = [0.6225939558611976, 0.31741844358468296, 0.30021947020232076, 0.2879767076234625, 0.2816196246580644, 0.278307210014324, 0.2706625106358769, 0.26596884293989703, 0.26113489119693484, 0.258862720444949, 0.2538508301732516, 0.2500978706762044, 0.24795926098871712, 0.2475700435614345, 0.2474988027654513, 0.2449054042197237, 0.240576413243708, 0.23794049446028892, 0.23579142852263016, 0.23416071618446196, 0.23175964105610897, 0.2281167431913241, 0.22967212850397284, 0.22777892890000584, 0.22390013767613304, 0.2207749385123301, 0.22007561708339538, 0.21906291023649352, 0.2202186444492051, 0.2183715919352541, 0.21624100915711336, 0.21618206422738354, 0.2144657770792643, 0.21527504010332954, 0.21418771004737025, 0.20980865606153853, 0.2126002414057953, 0.2084338887773379, 0.20881144506762725, 0.2098428104260955, 0.21015834108446585, 0.21072452598147923, 0.2085441920642901, 0.20655526348737754, 0.2065368036579604, 0.20730758721780296, 0.20318982825435775, 0.20558006308897578, 0.20451823486523193, 0.20272354262344766, 0.20406143171618682, 0.20186790056300885, 0.20383514862770985, 0.20294710015407716, 0.20312417983406722, 0.20129553070574097, 0.20182815419905115, 0.19995143821444175, 0.2002209018577229, 0.20048845356160944, 0.2000139168266094, 0.1996482022181906, 0.1971266975607535, 0.19796634134319094, 0.1976291211416023, 0.19831461519604981, 0.1977350798369658, 0.19696562391037892, 0.19740519635003023, 0.19580651338052268, 0.19552542419746669, 0.19521403026701223, 0.19553316933940154, 0.19488034660768028, 0.19368608198081605, 0.19446941344725965, 0.19412337966037518, 0.19310347359589855, 0.19327945871786636, 0.19149346393768232, 0.1913889961110221, 0.19279420729538407, 0.19179249125899692, 0.1918568325163138, 0.19269827331858452, 0.19066237191660235, 0.19030321953874646, 0.18820104093262643, 0.1888352728853322, 0.18865287996301747, 0.18994535106902172, 0.18779061983029047, 0.1879253023200565, 0.1878796937790784, 0.18680921370031858, 0.1877161840898822, 0.18765921687537973, 0.18688619324956277, 0.18712556745969888, 0.1881472183899446, 0.1868559956701115, 0.18793026199846558, 0.18603357582381277, 0.18570834338062941, 0.18664580613675744, 0.18676346645812797, 0.18560269091165427, 0.18392779550167046, 0.1848990312128356, 0.18445119645559427, 0.18404834461633604, 0.1850350255315954, 0.18390731340405916, 0.18369915449258054, 0.18213139820580532, 0.18245571317395778, 0.18334205049758007, 0.1808644284052078, 0.18406260871525967, 0.1822245866060257, 0.1826149041604514, 0.18292151685013916, 0.183117709527112, 0.18225026514494058, 0.18342482313664274, 0.1812764099149993, 0.18012011517779997, 0.18179725659917098, 0.18073263929949868, 0.18031359328465027, 0.17992877922575884, 0.18104041939733004, 0.18248498537624724, 0.18089871898745047, 0.18099285421347378, 0.17913860458918293, 0.17946353938543436, 0.17882485191027322, 0.1813056083640667, 0.17983627191396676, 0.17939712882342965, 0.1781468934031448, 0.18029704509359418, 0.17801467103488516, 0.18130464144427366, 0.1778514249005703, 0.17893372287954948, 0.1787083355164287, 0.17879480936310507, 0.17805866295039052, 0.1786693792903062, 0.17867714046227812, 0.17721971432970027, 0.17922249985764724, 0.1786921490924527, 0.18034540161941992, 0.17687382646883376, 0.1775592506082371, 0.17796364516922922, 0.17656348540325356, 0.17903954069120714, 0.177446384020526, 0.17612403804304624, 0.17612075512156342, 0.1769055115303608, 0.1758029433813962, 0.1770984482283544, 0.17653317990327122, 0.17646706412837962, 0.17723797698213598, 0.17599180957885704, 0.17597011235928295, 0.17764876037836075, 0.17514687964711526, 0.17577540234784889, 0.17570023810622667, 0.17458140887696333, 0.17483635984285914, 0.17625029294779807, 0.17440037022937427, 0.17631485034721067, 0.17500668816795253, 0.17459010009211723, 0.17416613834975947, 0.17438670925118707, 0.1735787997492636, 0.17299176602050512, 0.17324967900610933, 0.17171851801450808, 0.17390505299724715, 0.17360490477747387, 0.175390196343263, 0.17473318126767573, 0.17353339628739792, 0.17419342628934167, 0.17313677712221337, 0.1741275857343818, 0.17319951349436635, 0.1741999053593838, 0.17301984853816754, 0.1741177374214837, 0.17324945262887262, 0.17192250414930207, 0.17274447929377507, 0.1720627811972541, 0.17310017708576086, 0.1710192821543626, 0.17116620180883793, 0.17285357957536523, 0.17289781096306714, 0.17229877577887642, 0.17102467915927522, 0.16953032079971198, 0.16974883022332432, 0.17122587591710717, 0.17017784624388724, 0.17115415241381135, 0.1701657073666351, 0.17190006578510458, 0.16968380042699852, 0.17223402278290856, 0.17091652442409536, 0.1695584882840966, 0.17004712535576386, 0.16984767067914058, 0.1707475059712776, 0.17056746637881404, 0.16902377742408503, 0.17068739024677662, 0.17099577331482763, 0.17023768260924502, 0.17009319470386314, 0.16898854024181462, 0.1696141714399511, 0.1687036304913386, 0.16885564453674085, 0.16860849922052537, 0.16870775224283488, 0.16803243184330488, 0.16833909924584206, 0.16861127715821217, 0.1676489154648299, 0.16919665851376273, 0.1689386713986445, 0.16865605085787147, 0.16935986566423167, 0.16813740427746918, 0.1669035710469641, 0.17061894492368507, 0.16787589431712122]
val_loss = [0.3459715485572815, 0.319883993268013, 0.3013859083255132, 0.2829022198915482, 0.28288312753041583, 0.2807436943054199, 0.27288403511047366, 0.2661691496769587, 0.2649709979693095, 0.25727045238018037, 0.2516728103160858, 0.2503664940595627, 0.2556261450052261, 0.2529455175002416, 0.24732886056105297, 0.2540639191865921, 0.24455681443214417, 0.24238853255907694, 0.24227074086666106, 0.2317940413951874, 0.23161678810914357, 0.2338098684946696, 0.22701056798299155, 0.2257087101538976, 0.2286439190308253, 0.2243464211622874, 0.22703810036182404, 0.2211564650138219, 0.2172350913286209, 0.22598924537499746, 0.21419202586015065, 0.21806386709213257, 0.22137483259042104, 0.22227020064989725, 0.22146981259187062, 0.21803199052810668, 0.2184857000907262, 0.21310238540172577, 0.21789440910021465, 0.21190712749958038, 0.21347120503584543, 0.21749540368715922, 0.20941006342569987, 0.20270708898703257, 0.21187119384606679, 0.2107385238011678, 0.21085048963626227, 0.21041889488697052, 0.20468158523241678, 0.2061322182416916, 0.2148070196310679, 0.20736021051804224, 0.21348246236642202, 0.20537388821442923, 0.21083624362945558, 0.2070847272872925, 0.20757115383942923, 0.20380941182374954, 0.20241698871056238, 0.20468166172504426, 0.20410983463128407, 0.20662503242492675, 0.20152493417263032, 0.2065717081228892, 0.19427642474571863, 0.20712252855300903, 0.20105298161506652, 0.19875823557376862, 0.20692214022080105, 0.19839404424031576, 0.19953979005416234, 0.20071205943822862, 0.19746077011028926, 0.20186001062393188, 0.200666610399882, 0.2070266991853714, 0.1994818240404129, 0.1955285573999087, 0.20499415397644044, 0.2039321223894755, 0.19940469165643057, 0.19708466331164043, 0.19949062913656235, 0.19961571792761484, 0.20100903511047363, 0.1944386343161265, 0.20407972484827042, 0.194714426000913, 0.1987111250559489, 0.1980031241973241, 0.19096537232398986, 0.18998960504929224, 0.1941454937060674, 0.19400464047988256, 0.19663843313852947, 0.18902989427248637, 0.19303420037031174, 0.19039589911699295, 0.19198069473107657, 0.1914402276277542, 0.1935322105884552, 0.19332485993703205, 0.1918368697166443, 0.19230657269557316, 0.19363037794828414, 0.19123949905236562, 0.19592784146467845, 0.19744160374005634, 0.19086263875166576, 0.1946823606888453, 0.19380257825056713, 0.19487624913454055, 0.1890086680650711, 0.18607433885335922, 0.1867428407073021, 0.1863935003678004, 0.1885641152660052, 0.18909254769484202, 0.18820047080516816, 0.19420957962671917, 0.18767853180567423, 0.18991814355055492, 0.187985926369826, 0.1936057209968567, 0.18246346910794575, 0.18925666610399883, 0.18662190387646357, 0.18590016265710194, 0.18267055402199428, 0.18704833139975865, 0.18972358951965967, 0.18868599832057953, 0.18936549723148347, 0.19070831735928853, 0.1833306223154068, 0.1883013571302096, 0.18202077051003773, 0.18418795665105184, 0.18545178174972535, 0.18185614744822184, 0.19268637398878732, 0.1858741044998169, 0.18628669083118438, 0.18774417440096539, 0.19692918161551157, 0.18705802261829377, 0.1818427284558614, 0.18261725505193074, 0.1842357908686002, 0.1801128640770912, 0.18512797156969707, 0.1858647162715594, 0.1846560498078664, 0.1873978445927302, 0.18216014355421067, 0.18488996227582297, 0.18362624198198318, 0.18151661654313406, 0.1762937217950821, 0.1806035339832306, 0.18512091984351475, 0.18181119859218597, 0.18171793619791668, 0.18120009849468868, 0.1844463139772415, 0.18276222894589106, 0.18233428994814554, 0.18265171945095063, 0.18543657114108403, 0.18314156979322432, 0.18318595588207245, 0.18242125709851584, 0.18433914879957836, 0.17718965510527293, 0.1772746150692304, 0.18024184356133144, 0.18214445014794667, 0.1802961339553197, 0.18823419560988744, 0.17568106601635616, 0.1813238297899564, 0.17819047570228577, 0.1841731717189153, 0.17703190992275875, 0.18111002991596858, 0.17784021844466527, 0.1784808988372485, 0.17489933768908184, 0.1754055400689443, 0.17968564480543137, 0.18226555983225504, 0.17869227528572082, 0.1801154151558876, 0.18500609149535496, 0.17860538015762964, 0.1808014452457428, 0.18061185081799824, 0.18143119911352792, 0.1751203368107478, 0.17727649658918382, 0.1772987186908722, 0.18049545884132384, 0.17544150849183401, 0.17717508574326832, 0.17937145630518594, 0.1739457776149114, 0.17515501379966736, 0.17857180833816527, 0.18092521329720815, 0.17817211796840032, 0.17754695961872738, 0.1803257703781128, 0.17728886902332305, 0.18091382582982382, 0.17437865684429804, 0.1750065972407659, 0.17586680104335148, 0.17914748638868333, 0.18333296676476796, 0.1761213983098666, 0.1832310731212298, 0.17868048201004663, 0.17486719638109208, 0.16951336314280827, 0.18033902843793234, 0.1793589567144712, 0.1778097927570343, 0.17770484387874602, 0.17876331160465878, 0.17465585470199585, 0.17253190378348032, 0.17125427424907685, 0.17953084856271745, 0.1771846145391464, 0.17789531350135804, 0.1742671400308609, 0.173278413216273, 0.17398502628008525, 0.17347124765316646, 0.17500176976124446, 0.17379520932833353, 0.17322017947832744, 0.17468914836645127, 0.17813160916169485, 0.17557876855134963, 0.1739778180917104, 0.17701848844687143, 0.16964496870835621, 0.17520492523908615, 0.16997664372126262]
plt.figure()
plt.plot(loss, label='loss')
plt.plot(val_loss, label='val_loss')
plt.legend()
plt.ylim(0,0.5)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('Training and validation loss')
plt.show()

# %%
